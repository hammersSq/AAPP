# Definition
Dynamic programming is an algorithmic paradigm: it aim to simplifying complicated problem by breaking it down into simpler sub-problems in a recursive manner.
- So in order to apply dynamic programming it has to have an **optimal sub-structure**
It sounds very similar to the Divide and Conquer paradigm but it differ in one particular:
- in order for dynamic programming to be applicable the problem we want to solve must have **overlapping sub-problems.**
## Overlapping sub-problems
_Overlapping_ sub-problems means that the space of sub-problems must be small, that is, any recursive algorithm solving the problem should solve the same sub-problems over and over, rather than generating new sub-problems.
This is exploited by dynamic programming, storing the result of the sub problems to be reused when needed.

## Optimal sub-structure
Optimal substructure means that the solution to a given optimization problem can be obtained by the combination of optimal solutions to its sub-problems. Such optimal substructures are usually described by means of recursion.

## Memoization
Memoization is the Top Down approach for dynamic programming:After computing a solution to a sub-problem, store it in a table. Subsequent calls check the table to avoid redoing work.

# Longest Common Subsequence
The Longest Common Subsequence is an example of a problem that can be solved with dynamic programming.

## The problem
Given two sequence **x[1 , 2, ..., m]** and **y[1, 2, ..., n]** you have to find **a** longest subsequence common to them both. I.e **you have to find a subsequence, common to them both with the maximum length (between all the subsequence)**

![[Pasted image 20231102171523.png]]

## Brute-force Algorithm
A first idea can be use brute force:
- Check every subsequence of x[1, 2, ..., m] to see if it is also a subsequence of y[1, 2, ..., n]

This is not a very convenient way to solve the problem, in fact:
- to check a subsequence we need O(n) time
- there are 2<sup>m</sup> subsequences of x 
So, in the worst case, we have a running time = O(n2<sup>m</sup>) ->exponential time!

## Look fomnr the length of the LCS
Find the length of the LCS between x and y is a simpler problem, but it can be used in order to find the LCS itself.
From now on we denote the length of a subsequence **s** as **|s|**.
Lets define:
- c[i,j]=|LCS(x[1,...,i], y[1,...,j])| -> LCS between the sub string of x and y starting from 1 to i (for x) and from 1 to j(for y).
- c[m,n]=|LCS(x,y)| -> it is natural if we look how we define c[i,j]

c[i,j] can be defined in a recursive way:
![[Pasted image 20231102173450.png]]


==This is the sign of an **Optimal sub-structure**==


### Algorithm for the length
Algorithm that exploit the recursive definition of c[i,j]
```pseudocode
LCS(x, y, i, j)//ignoring the base case
  // Check if the current elements match
  if x[i] == y[j]:
    c[i, j] = LCS(x, y, i - 1, j - 1) + 1
    //shift both x and y, add 1 in the lenght
  else:
    c[i, j] = max(LCS(x, y, i - 1, j), LCS(x, y, i, j - 1)) 
	    //we invoke the algorithm shifting on x and on y then we keep
	    the max

  return c[i, j]
  ```
Worse case: x[i] != y[ j], in which case the algorithm evaluates two sub-problems, each with only one parameter decremented.

### Intuition
Noting anything weird? **A lot of sub problem are solved more than one time!**
![[Pasted image 20231102174559.png]]

==And this is the sign of **Overlapping sub-problems**==


### Memoization solution 
Once we have computed c[i,j], for some i,j we can easily store it in c[ i, j] : when it is requested to calculate c[i,j] again, before to do it we check if it is already done.
```pseudocode
LCS(x, y, i, j)
  if c[i, j] = NIL
    if x[i] = y[j]
      c[i, j] = LCS(x, y, i - 1, j - 1) + 1
    else
      c[i, j] = max(LCS(x, y, i - 1, j), LCS(x, y, i, j - 1))
  return c[i, j]
  ```
After checking if c[i,j] have been already calculated the code is the same of before.
Time cost= Theta(mn)
Space cost= Theta(mn)

## LCS Algorithm
We can use the table built in the calculation of the length of LCS to build a longest subsequence:
![[Pasted image 20231102181234.png]]
You can simply go bottom up:
- you start from the end of the matrix c[n,m] ->here you can look up or left, if you look up you chose B as the last char of LCS else you chose A
- considering we go up: here we have an item that is in common, when this is the case we go up and left
- when x[i]!= y[j] we go in a cell where the value is the same (our case up)
- and so on until we reach a cell with zero, we have built our lCS


**Time Complexity**= Theta(mn)
**Space complexity**= Theta(mn)
**Reconstruction Time Complexity**= O(min{m,n}) -> it does not dominate
