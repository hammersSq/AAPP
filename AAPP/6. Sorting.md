# Quick Sorting

The quick sort is a [[3.Divide and Conquer|Divide and Conquer]] algorithm use to sort elements in a list.
It sort the array **"in place"**, so using only the existing data structure.

The 3 principal phase of this algorithm are:
![[Pasted image 20231003143133.png]]
quick sort is a divide and conquer approach, combine is trivial, we do not to do anything.
is important to divide the part in a good way...the divide part has to be of the lowest complexity as possible,  linear. if we have linear divide part the algo will be very efficient.

## Partitioning
This phase is very important and we have to solve it in the minimum running time: O(n)!
```Python
def partition(A, p, q):
    pivot=A[p]
    i=p
    for j in range(p+1,q+1):
        if (A[j] <=pivot):
            i=i+1
            x=A[i]
            A[i]=A[j]
            A[j]=x
    
    x=A[i]
    A[i]=A[p]
    A[p]=x
    print(i)
    return i
```
Notice that this sub-routine work always on the same array, but on different part of it.

## Quicksort pseudo-code 
Once defined the partitioning the quick sort pseudo-code is:
```python
def sort(A, p, r):
    if(p<r):
        q=partition(A, p ,r)
        sort(A,p,q-1)
        sort(A,q+1,r)
```
The initial call of the QUICKSORT is 
**quicksort(A,1,n)**
so the initial pivot is 1.

## Quicksort Analysis
### Worst case analysis
Let's start analyzing the worst case of the quicksort: **the element in the list are sorted in the reverse order.**
If this is the case after the first partition we have that the pivot is the maximum of the array and so the next call of the quick sort will work on two list, one of size n-1 and one of size 0! This lead to this recurrence formula:
**T(n)=T(0)+T(n-1) +theta(n)** 
where theta is the time fort partitioning.
Solving this equation using arithmetic series bring to have a running time complexity:
**theta(n<sup>2</sup>**)
![[Pasted image 20231003144445.png]]

### Best case Analysis
The best case is when the first call of partition sub-routine (and all the next one) split the array in two array with the same size.
In this case we will have this recurrence forumula:
**T(n)=2T(n/2) +theta(n)**
that can be solved with the [[2. Analysis of algorithms#The Master Method |Master Method]]:
a=2
b=2
log<sub>2</sub>(2)=1
f(n)=partition theta(n)
T(n)=n log(n)
![[Pasted image 20231003145126.png]]

### An intuition
If we build a case when we alternate lucky and unlucky case (one case split in 2 equally sized part and the other one is the worst case), and then we solve the equation we can see that we fall in the lucky case:
![[Pasted image 20231003145137.png]]
This can let we think that there are many case in which this algorithm asymptotically tend to n log(n).

**Is there a way that make me sure that in general we have the lucky case?** we can introduce the randomization in order to pick the pivot at random: since the pivot is chosen at random, it is very unlikely that we chose a pivot in a way that one of the 2 part of the array that are generated is empty.

## Randomized quicksort
![[Pasted image 20231003145739.png]]
This algorithm will work with a n log(n) complexity in the average case, that is the most frequent one! 
==E[T(n)] < a n log(n) for a constant a >0== 

What we have to change from the initial quick sort is only the partition function:
```python
def randomized_partition(A, p, q):
    i = random.randint(p, q)  # Choose a random pivot index
    A[p], A[i] = A[i], A[p]  # Swap the pivot element to the first position
    pivot = A[p]
    i = p
    for j in range(p+1,q+1):
        if (A[j] <=pivot):
            i=i+1
            x=A[i]
            A[i]=A[j]
            A[j]=x
    
    x=A[i]
    A[i]=A[p]
    A[p]=x
    print(i)
    return i
```

#### Complexity of the Randomize quicksort
Let T(n) be the random variable for the running time of randomized quicksort on an input size of n, assuming that random numbers are independent.
for **k=0 ,1 ,.. ,n-1**  let's define the [[5.Randomized Based Algorithms#Tools#Indicator Variables|Indicator Variables]] **X<sub>k</sub>**:
![[Pasted image 20231028112506.png]]
These variables are going to describe which kind of partition we may have, instead of analyzing a specific event we are going to analyze a class of event:
whatever is the pivot element we differentiate the possible case with the respect to the partition we obtain.

define **T(n)** as the summation of the set of possible recursion. 
![[Pasted image 20231003150228.png]]
Each recursion is equally probable, and depends on the pivot that we have selected.
The all indicator variable are independent, so we can do these calculation:
![[Pasted image 20231003150426.png]]
![[Pasted image 20231003150503.png]]
==Using the **Hairy Recurrence** you can prove that:
E[T(n)] < a n log(n) for a constant a >0== 


## Quick sort in practice
In practice:
- Quick sort is a **great general purpose sorting algorithm**
- Is typically **over twice as fast as merge sort**
- quick sort can benefit substantially from **code tuning** 
- it behaves well even with caching and virtual memory

# Best complexity for sorting
We have seen different algorithm for sorting [[2. Analysis of algorithms#Insertion sort|Insertion sort]], [[2. Analysis of algorithms#Merge Sort|Merge sort]],  [[6. Sorting#Quick Sorting |Quick sort]]and the best-worst case that we have seen is **O(n log(n))**.
Is this the best that we can do?
The answer is no, and now we are going to prove this with the help of **Decision Trees**.
## Decision tree proof
The idea is to describe the sorting algorithm in a very abstract way: in every sorting algorithm you are going to pick 2 values, you are going to compare these two elements and according to the sorting comparison function you may do different things. 

### Example
![[Pasted image 20231028120444.png]]
Each internal node is labeled i:j for i, j in {1, 2,..., n}.
• The left sub-tree shows subsequent comparisons if ai <= aj.
• The right sub-tree shows subsequent comparisons if ai > aj.

Each leaf contains a permutation <pi(1), pi(2),..., pi(n)> to
indicate that the ordering a<sub>pi(1)</sub> <= a<sub>pi(2)</sub> <= ... <= a<sub>pi(n)</sub> has been
established.

### Decision tree model
If you think, in general you may built this tree for every algorithm. 

• One tree for each input size n.
• View the algorithm as splitting whenever it compares two elements.
• The tree contains the comparisons along all possible instruction traces.
• The running time of the algorithm is the length of the path taken.
• Worst-case running time is the height of tree

#### Theorem
Any decision tree that can sort n elements must have height Theta(n lg n) .

**proof**: The tree must contain >= n! leaves, since there are n! possible permutations. A height-h binary tree has <= 2<sup>h</sup> leaves. Thus, n! <= 2<sup>h</sup> (this is the upper bound of the number of leaves)

now solving the logarithmic inequality we are going to find upper and lower bound for the height of the decision tree:
![[Pasted image 20231028121820.png]]
**Corollary**: Heap-sort and merge-sort are asymptotically optimal comparison sorting algorithms.

### Sorting in linear time, Counting sort

![[Pasted image 20231028122124.png]]
The third point is saying that k is that k is the maximum value we are going to store in the array.

**Pseudo code**:
![[Pasted image 20231028122214.png]]
**Explanation**:
1. At the beginning we are going to put 0 in all the position of the auxiliary array C, that has a number of elements equal to the maximum value in the initial array 
![[Pasted image 20231028122803.png]]
3. then we are going to count how many times an algorithm is in the array
![[Pasted image 20231028122825.png]]
5. Then we do the  [[4.Parallel Machine Model#Prefix sum|Prefix Sum]]: this start from the second item, and we sum the previous one and go on with the next one..
![[Pasted image 20231028123045.png]]
6. Then we start looking to the last item in the initial array, then it look in the auxiliary array, that says us the final destination of that item. then we decrement that value in the auxiliary array. this is done for all item in the initial array.
![[Pasted image 20231028123243.png]]
#### Complexity
![[Pasted image 20231028123459.png]]
if k is O(n) counting sort takes Theta(n) time!

how it is possible that is linear? we proof that the best one is Theta (n log(n))? this is possible because we are![[Pasted image 20231028124042.png]]not exploiting the comparison, so there is no decision tree.
so: **Any comparison based sorting algorithm has a time complexity of Theta (n log(n)** ,but if you don' t use comparison you can have better time complexity

**problem**: this algorithm use a lot of memory! in order of GB only to sort a few position integer array.

#### Stable Sorting property
the counting sort is **stable**, this means that it preserves the input order among equal elements.

# Radix sort
## Idea
Sort on least-significant digit first with auxiliary stable sort, for example you can use counting sort.
![[Pasted image 20231029195343.png]]
you first order you data using the less significant digit, then you pass to the more significant and so on. Due to the fact that you are using a stable sort the order obtained in the previous ordering operation is maintained. 

## Analysis of the Radix Sort
This analysis is done assuming that the counting sort is the auxiliary stable sort algorithm and that we want to sort **n** Word of **b** bits each.
Each word can be viewed as having **b/r** base **2<sup>r</sup>** digits:

![[Pasted image 20231029195914.png]]
Counting sort takes Theta(n + k) time to sort n numbers in the range from 0 to k-1.
If each b-bit word is broken into r-bit pieces, each pass of counting sort takes Q(n + 2<sup>r</sup>) time. Since there are b/r passes, we have:
**T(n,b)= Theta (b/r (n+2<sup>r</sup>)**

If we chose **r=log(n)** :
![[Pasted image 20231029200233.png]]

## Conclusion
In practice, Radix sort is fast for large inputs, as well as simple to code and maintain.
Example (32-bit numbers):
• At most 3 passes when sorting >=2000 numbers.
• Merge sort and quicksort do at least lg 2000= 11
passes.









