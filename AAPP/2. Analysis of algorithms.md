# Key Points
- Definition of Algorithm
- Algorithm runtime
- Theta Notation
- Substitution method
- Master Method

# Algorithm Analysis
**What is an algorithm?** ==An algorithms is any well-defined computational procedure that takes some value, or set of values, as input and produces some value, or set of value, as output.==
It is important to notice that an algorithm **must** terminate in a finite number of steps.

Analyzing an algorithm means to think if:
- if exists an algorithm that solve a certain problem
- how long does it takes to solve it
- if it is possible to parallelize it 
- how much memory do you need
- (...)

# Algorithms and performance
It is important to think about the performance of the algorithm, analyzing them allow us to draw a line between what is feasible and what is not and to understand how much it costs to implement some action (time is money, resource are money).

## Running time
==**Running Time** is a term used in computer science to refer to the time required by an algorithm to complete the execution of a specific operation.==
The running time is strictly related to the input, it is possible that the same algorithms has different running time when we change the input (e.g. a sorted list is easier to sort, so it takes less time).
Since the running time is related to the input it is important, in order to do effective analysis, to parameterize the it by the size of the input(shorter input in general lead to have a smaller running time):
T(n) -> time take from the algorithm to execute on a input of size n.

When analyzing an algorithm in general we are looking for upper-bounds of running time, since they are useful to give guarantee.

## Kinds of analysed
When analyzing the cost algorithm we can look at it in several way:
- **Worst Case**: **T(n)**=maximum time of an algorithm on any input of size **n**
- **Average Case**: **T(n)**= expected time of an algorithm over all inputs **n**. This need assumption of statistical distribution of inputs.
- **Best Case**: **T(n)**= the minimum time of an algorithm on a specific input of time **n**. This can be useful to analyze which algorithm to use when we know in advance the input.
- 
To have an overview of the cost of the algorithms we cannot rely on seconds, because if we have a more powerful computer it is logical that an algorithm need less time. So it is important to have a concept of time that is independent by the machine we are using.
An idea can be using as a time the number of operation that the algorithms need do perform, so:
**T(n)**= number of operation that the algorithms perform on a input with size **n**.

## Asymptotic Analysis
Ignoring the machine-dependent constant we want to look how **T(n)** grows when n goes to infinity.
To do this here are three notation that can help us.
### O Notation
**O(g(n))**={f(n): there exist constants c > 0, n0 > 0 such that 0 <= f(n) <= cg(n) for all n>=n0 }
O notation define an upper bound for a function
example:
2n<sup>2</sup> belong to O(n<sup>3</sup>)

if we do not want to include "=" in the formula we can use o(n) that is the same but f(n) is strictly less than g(n)

### Omega notation
**Ω(g(n))** = { f(n) : there exist constants c > 0, n0 > 0 such that 0
<= cg(n) <= f(n) for all n >= n0 }
example:
√n=Ω(lg (n))

if we do not want to include "=" in the formula we can use ω(n) that is the same but f(n) is strictly greater than g(n)
### Theta Notation
**Θ(g(n))**={f(n): there exists positive constants c1, c2, and n0>0 such that 0 <= c1g(n) <= f(n) <= c2g(n) for all n>=n0}. In practice this means that if we have a function we can ignore low-order terms:
Example: 3n<sup>3</sup>+90n<sup>2</sup> -5n +6047= Θ(n<sup>3</sup>)

#### Comparing algorithms using theta notation
When n gets large enough, a Θ(n<sup>2</sup>) algorithm always beats a Θ(n<sup>3</sup>) algorithm.
![[Pasted image 20230927145352.png]]
In general we should ignore asymptotically slower algorithm, however real-world design situation often call for a careful balancing of engineering objectives. 
Asymptotic analysis is useful tool to help to structure our thinking.

It is important to notice that if  f(n) is Θ(g(n)) then f(n) is also O(g(n) and Ω(g(n).

# Case of studies
## Insertion sort 
The pseudo-code:
![[Pasted image 20230927151923.png]]

**Analysis**
![[Pasted image 20231012103049.png]]
Worst case = Average Case = Θ(n<sup>2</sup>)
Is the insertion sort a fast algorithm for sorting?
it is moderately fast for small, but it is very slow on large n.

## Merge Sort
It is a recursive algorithm, how it works:
![[Pasted image 20230927152848.png]]
![[Pasted image 20231012103209.png]]
**Analysis**
To analyze the complexity of the merge sort we have to understand how much it phases of the sorting costs:
![[Pasted image 20230927153012.png]]
If we have to sort a list with 1 element (base case) naturally it costs 1.
if the lists has more than 1 element there is the cost of splitting the list (theta(1)), the cost of sort the 2 sub-list (2 T(n/2)) and then the cost of merge the 2 sub-list(theta(1)).
![[Pasted image 20230927154818.png]]

Solving the equation looking at the recursion tree
![[Pasted image 20230927155213.png]]

So we have that merge sort costs theta(n lg n) that grows slowly than theta(n<sup>2</sup>). (the algorithm goes down in the tree for all n elements)
So merge sort asymptotically beats insertion sort in the worst case.
there are situation where insertion sort beat merge sort(n<30), in some case we have to consider also this!

# Solving recurrences
Recurrences are equation that describe the time or the space used by an algorithm in function of the size of the input.
## Substitution Method
![[Pasted image 20231012103729.png]]
## The Master Method
The master method is a method to calculate T(n)  where we have recurrences of the form:
T(n)= a T(n/b) + f(n)
where a >= 1, b>1 and f is asymptotically positive

We can distinguish between 3 common cases, the complexity of these case is related to the complexity of the f(n) function
1. f(n)=O(n <sup>log<sub>b</sub>a -epsilon</sup>)for some epsilon >0 -> T(n) = theta(n <sup>log<sub>b</sub>a </sup>)
2. f(n)=theta(n <sup>log<sub>b</sub>a </sup> lg<sup>k </sup>n) -> T(n)= theta(n <sup>log<sub>b</sub>a </sup>lg<sup>k +1</sup>n)
3. f(n)=omega(n<sup>log<sub>b</sub>a  + epsilon</sup>) for some epsilon >0 -> T(n)= theta(f(n))
Notice that b and a are constant defined by the recursive formula
It is important to notice that the Master Method can be used not only to
calculate the Time of the execution, but all costs (E.G. memory cost)

## Example

![[Pasted image 20230927161005.png]]

![[Pasted image 20230927161024.png]]
