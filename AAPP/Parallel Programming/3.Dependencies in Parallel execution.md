Parallel execution from any point of view will be constrained by the sequence of operations needed to be performed for a correct result.
Parallel execution must address control, data and system dependencies.
**A Dependencies arise when an Operation A depends on an earlier Operation B to complete and produce a result before A can be performed**.
This notion of dependency is extended to resources since some operation may depend on certain resources: for example due to where the data is located.

# Sequential consistency
When executing two statements in parallel (on two different processor) the processors execute independently from each other, so the order of the execution is not know at priori.
We can say that **Sequential consistency** is satisfied if:
1. Statement execution does not interfere with each other
2. Computation result is independent from the order of execution


# Independent vs Dependent
If we have two statement, A and B and executing in the order:
1. A
2. B
is equivalent to
1. B
2. A
**The two states are independent**
Two state are **dependent** when the order of their execution affects the computation outcome

# Type of dependencies 
## Flow Dependence
Also know as **true dependence**, arise in situation like this:
S1: a=1
S2: b=a
S2 is dependent from S1.
**You cannot remove this type of dependency**
==S2 has a flow dependence on S1 if and only if S2 read a value written by S1==
## Output Dependence
Arise in situation like this:
S1: a= f(x)
S2: a=b
second is dependent from the first
The problem here is that the order of execution will impact on the content of a.
If f has no side effect you can remove S1 from the flow, otherwise execute f(x) but does not assign the return value to the variable a
S1: f(x)

==S2 has an output dependence on S1 if and only if S2 writes a variable written by S1==

## Anti Dependence
Arise in situation like this:
S1: a=b
S2: b=1
first is dependent from the second.
To remove this dependence you can give another name to b:
S1 : a=b1
S2: b2=1 
this kind of thing can be easily done by the compiler.
==S2 has a anti dependence on S1 if and only if S2 writes a value read by S1==

## Statement Dependency graph
The statement Dependency graph is an useful tool to show dependence relationships:
![[Pasted image 20231208184657.png]]

Statement S1 and S2 can execute in parallel if and only if there are no dependencies between S1 and S2. We need to understand the code and remove those dependencies (if possible) to run the two statements in parallel.
## Find dependences
we can define two set:
- **IN(S)**: set of memory locations that may be used in S
- **OUT(S)**: set of memory location that may be modified by S
we can define :
- Flow(S1,S2)= intersection between OUT(S1) and IN(S2)
- Anti(S1,S2)=intersection between IN(S1) and OUT(S2)
- Output(S1,S2)= intersection between OUT(S1) and OUT(S2)

# Loop level parallelism and dependencies 

## Do All loop
A **Do All Loop** is a loop where all iterations are independent of each other (only the computation of the index of the loop is dependent, but we can unroll the loop and replace the index with the actual value).
![[Pasted image 20231208222408.png]]

If all iterations are independent we can ideally execute them in parallel at the same time (but this depend also by other things like the language we are using or the actual architecture, also you have to think about the costs of create threads and synchronize them: parallelization may be not worthy).
In the case of Do All loop you have linear speedup!

## Loop with dependencies

**Loop carried dependence**: it is a dependence where the two dependent statements belong to different iterations of the same loop. we say that the dependence is **lexically forward** if the source comes before the target or **lexically backward** otherwise.
### Case 1
![[Pasted image 20231208222840.png]]
In the **Case 1** we have that the iteration i has a flow dependences with iteration i-1
If this is the case there is no way to parallelize this.

### Case 2
![[Pasted image 20231208222936.png]]
In the case 2 we have that iteration i has a flow dependences with iteration i+5.
In this case we can do a little bit of parallelization:
- we can have a task for each chain, executing so 5 iteration at the time.

### Case 3
![[Pasted image 20231208225008.png]]
In this case we have a dependences between i and i-1, but after the execution of S1 in iteration 1 we can parallelize the execution of S2 in iteration i and S1 in iteration i+1, starting from i=1 to i=100.
This is a sort of pipeline.

### Case 4
![[Pasted image 20231208225531.png]]
In this case the outer loop (index i) is loop-independent but the inner one has a loop-carried dependence on j:
- We can parallelize the i iteration on the 100 inner loop, so having 100 thread that does 100 instructions

### Case 5
![[Pasted image 20231208225745.png]]
This look similar to the previous one, but in this case the outer loop cannot be parallelized only the inner one:
- we have to finish the 100 iteration of the inner loop before passing to the next 100 iteration
