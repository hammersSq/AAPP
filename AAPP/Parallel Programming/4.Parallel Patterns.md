A **Parallel patterns** is a recurring combination of task distribution and data access that solves a specific problem in parallel algorithm design. 
The patterns can help us to create a good parallel algorithm: sometime we can nest together patterns to obtain our algorithm.
![[Pasted image 20231212155339.png]]

Nesting is useful because allows other patterns to be composed
in a hierarchy so that any task block in the above diagram can be replaced with a pattern with the same input/output and dependencies.
We can think at the nesting as a Pattern itself.
# Serial Control patterns
Pattern does not exists only for Parallel programming, but also for Serial programming.
Those are the most important serial control patterns:
- Sequence
- Selection
- Iteration
- Recursion
It is important to know and understand them because some time (if the [[3.Dependencies in Parallel execution|dependencies]] allow us to do that) we can parallelize them.
## Sequence 
The **Sequence** is a list of task that is executed in a specific order:
![[Pasted image 20231212160235.png]]
The specification of the task says that all the statements must be executed together, but if there are no dependencies between some task we can execute them in a parallel way:
![[Pasted image 20231212160352.png]]
## Selection
**Selection** is a very common pattern: we have a condition **c** that is first evaluated. Then, depending on the result of the condition we execute either task **a** or **b**

If the task a and b does not depend on the execution of c we can parallelize this executing a,b,c together and then choose the output depending on the result of c (this is a common practice in hardware design).
![[Pasted image 20231212161706.png]]

## Iteration
In **iteration** a condition c is evaluated. If true, a is evaluated and c is evaluate again. this repeats until c is false.

We have already seen that parallelize this can arise problem when there are [[3.Dependencies in Parallel execution#Loop with dependencies |loop with dependencies]].
![[Pasted image 20231212161724.png]]

## Recursion
Dynamic form of nesting allowing functions to call themselves.
Tail recursion is a special recursion that can be converte into iteration, allowing us to parallelize it.
We can also parallelize several invocation of the function like in the [[3.Divide and Conquer#Merge sort||merge sort]].


# Parallel patterns
Parallel patterns extends serial control patterns. There are several parallel patterns and we will go through each of them.

## Fork-join
The **fork join** pattern allows to split the control flow into multiple flow that rejoin later.
An example of implementation is the one of **click plus**:
1. The call tree is a parallel call tree and where functions are spawned instead of called
2. Function that spawn another function call will continue to execute
3. caller **syncs** with the spawned function to join the two
Notice that the syncs is different than a barrier: in syncs only one thread will continue, with a barrier all thread will continue.

## Map
The pattern **map** work on a collection of elements: it will perform a function on every element of the collection.
Map replicates a serial iteration pattern, where:
- each iteration is independent from the others
- the number of iteration is known in advance 
- computation only depends on the iteration count and data from input collection
The replicated function is referred to as **elemental function**.
The map pattern can be applied if the elemental function can be applied to each element without knowledge of the neighbor.
Also the elemental function must be **pure**, meaning that it must not modify the shared state.

**Examples**:
![[Pasted image 20231212170848.png]]
The map pattern is embarrassingly parallel, since there is the lack of dependences we can run all the invocation of the elemental function in parallel obtaining a significant speedup:
If we think at the [[4.Parallel Machine Model#Amdahl's law|Amdahl's law]] there is no fraction related to the serial execution. 

### Sequential vs parallel map
If we think a map as a sequential it si implemented in this way:
![[Pasted image 20231212174338.png]]
In the sequential case we have that each task execute after the one related to the previous element in the collection ends.
In the parallel case instead we have that each calls of the elemental function run together:
![[Pasted image 20231212175036.png]]
In the above images we are assuming that all the task take the same time but this is not always the case: can happen that the time taken from the task depends from the input so you need a barrier a the end to wait all task to finish.

### N-ary Map
The map we have seen until now is called **unary map**, where we have 1 input collection and 1 output collection.
Some case it is useful to have several input collections, and this type of map is called **n-ary maps**.
**Example**:
![[Pasted image 20231212180045.png]]

### Sequence of maps Optimization
Now we will see some optimization in the case we have a sequences of maps like this:
![[Pasted image 20231212180512.png]]
A naive implementation of this may just write in a intermediate result to memory. This is a very easy implementation but in this way you will waste memory Band Width  and likely overwhelming the cache (this badly affect performance).

#### Code fusion
This implementation will fuse the elemental function of the several map into one big elemental function. This will adds arithmetic intensity but reduces the memory/cache usage. 
![[Pasted image 20231212181332.png]]
This will improve the performance if you can implement the big elemental function that use only the registers and do not rely on memory, if you have a lot of memory involved this is more and less useless.
#### Cache fusion
Sometimes you cannot fuse together the map operations. What you can do is split the work into block, giving one block for each cpu.
![[Pasted image 20231212182040.png]]
Hopefully, since each cpu work only with one block it will find the result of a map step in the cache and it can use it for the next map step.
Since caches are faster than memory this will have better performance from the naive one.

//TODO stencil, work pile, divide and conquer\

