A **Parallel patterns** is a recurring combination of task distribution and data access that solves a specific problem in parallel algorithm design. 
The patterns can help us to create a good parallel algorithm: sometime we can nest together patterns to obtain our algorithm.
![[Pasted image 20231212155339.png]]

Nesting is useful because allows other patterns to be composed
in a hierarchy so that any task block in the above diagram can be replaced with a pattern with the same input/output and dependencies.
We can think at the nesting as a Pattern itself.
# Serial Control patterns
Pattern does not exists only for Parallel programming, but also for Serial programming.
Those are the most important serial control patterns:
- Sequence
- Selection
- Iteration
- Recursion
It is important to know and understand them because some time (if the [[3.Dependencies in Parallel execution|dependencies]] allow us to do that) we can parallelize them.
## Sequence 
The **Sequence** is a list of task that is executed in a specific order:
![[Pasted image 20231212160235.png]]
The specification of the task says that all the statements must be executed together, but if there are no dependencies between some task we can execute them in a parallel way:
![[Pasted image 20231212160352.png]]
## Selection
**Selection** is a very common pattern: we have a condition **c** that is first evaluated. Then, depending on the result of the condition we execute either task **a** or **b**

If the task a and b does not depend on the execution of c we can parallelize this executing a,b,c together and then choose the output depending on the result of c (this is a common practice in hardware design).
![[Pasted image 20231212161706.png]]

## Iteration
In **iteration** a condition c is evaluated. If true, a is evaluated and c is evaluate again. this repeats until c is false.

We have already seen that parallelize this can arise problem when there are [[3.Dependencies in Parallel execution#Loop with dependencies |loop with dependencies]].
![[Pasted image 20231212161724.png]]

## Recursion
Dynamic form of nesting allowing functions to call themselves.
Tail recursion is a special recursion that can be converte into iteration, allowing us to parallelize it.
We can also parallelize several invocation of the function like in the [[3.Divide and Conquer#Merge sort||merge sort]].


# Parallel patterns
Parallel patterns extends serial control patterns. There are several parallel patterns and we will go through each of them.

## Fork-join
The **fork join** pattern allows to split the control flow into multiple flow that rejoin later.
An example of implementation is the one of **click plus**:
1. The call tree is a parallel call tree and where functions are spawned instead of called
2. Function that spawn another function call will continue to execute
3. caller **syncs** with the spawned function to join the two
Notice that the syncs is different than a barrier: in syncs only one thread will continue, with a barrier all thread will continue.

## Map
The pattern **map** work on a collection of elements: it will perform a function on every element of the collection.
Map replicates a serial iteration pattern, where:
- each iteration is independent from the others
- the number of iteration is known in advance 
- computation only depends on the iteration count and data from input collection
The replicated function is referred to as **elemental function**.
The map pattern can be applied if the elemental function can be applied to each element without knowledge of the neighbor.

**Examples**:
![[Pasted image 20231212170848.png]]
The map pattern is embarrassingly parallel, since there is the lack of dependences we can run all the invocation of the elemental function in parallel obtaining a significant speedup:
If we think at the [[4.Parallel Machine Model#Amdahl's law|Amdahl's law]] there is no fraction related to the serial execution. 
