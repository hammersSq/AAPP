A **Parallel patterns** is a recurring combination of task distribution and data access that solves a specific problem in parallel algorithm design. 
The patterns can help us to create a good parallel algorithm: sometime we can nest together patterns to obtain our algorithm.
![[Pasted image 20231212155339.png]]

Nesting is useful because allows other patterns to be composed
in a hierarchy so that any task block in the above diagram can be replaced with a pattern with the same input/output and dependencies.
We can think at the nesting as a Pattern itself.
# Serial Control patterns
Pattern does not exists only for Parallel programming, but also for Serial programming.
Those are the most important serial control patterns:
- Sequence
- Selection
- Iteration
- Recursion
It is important to know and understand them because some time (if the [[3.Dependencies in Parallel execution|dependencies]] allow us to do that) we can parallelize them.
## Sequence 
The **Sequence** is a list of task that is executed in a specific order:
![[Pasted image 20231212160235.png]]
The specification of the task says that all the statements must be executed together, but if there are no dependencies between some task we can execute them in a parallel way:
![[Pasted image 20231212160352.png]]
## Selection
**Selection** is a very common pattern: we have a condition **c** that is first evaluated. Then, depending on the result of the condition we execute either task **a** or **b**

If the task a and b does not depend on the execution of c we can parallelize this executing a,b,c together and then choose the output depending on the result of c (this is a common practice in hardware design).
![[Pasted image 20231212161706.png]]

## Iteration
In **iteration** a condition c is evaluated. If true, a is evaluated and c is evaluate again. this repeats until c is false.

We have already seen that parallelize this can arise problem when there are [[3.Dependencies in Parallel execution#Loop with dependencies |loop with dependencies]].
![[Pasted image 20231212161724.png]]

## Recursion
Dynamic form of nesting allowing functions to call themselves.
Tail recursion is a special recursion that can be converte into iteration, allowing us to parallelize it.
We can also parallelize several invocation of the function like in the [[3.Divide and Conquer#Merge sort||merge sort]].


# Parallel patterns
Parallel patterns extends serial control patterns. There are several parallel patterns and we will go through each of them.

## Fork-join
The **fork join** pattern allows to split the control flow into multiple flow that rejoin later.
An example of implementation is the one of **click plus**:
1. The call tree is a parallel call tree and where functions are spawned instead of called
2. Function that spawn another function call will continue to execute
3. caller **syncs** with the spawned function to join the two
Notice that the syncs is different than a barrier: in syncs only one thread will continue, with a barrier all thread will continue.
![[The-fork-join-model-used-for-thread-level-parallelism-in-OpenMP.png]]

## Map
The pattern **map** work on a collection of elements: it will perform a function on every element of the collection.
Map replicates a serial iteration pattern, where:
- each iteration is independent from the others
- the number of iteration is known in advance 
- computation only depends on the iteration count and data from input collection
The replicated function is referred to as **elemental function**.
The map pattern can be applied if the elemental function can be applied to each element without knowledge of the neighbor.
Also the elemental function must be **pure**, meaning that it must not modify the shared state.

**Examples**:
![[Pasted image 20231212170848.png]]
The map pattern is embarrassingly parallel, since there is the lack of dependences we can run all the invocation of the elemental function in parallel obtaining a significant speedup:
If we think at the [[4.Parallel Machine Model#Amdahl's law|Amdahl's law]] there is no fraction related to the serial execution. 

### Sequential vs parallel map
If we think a map as a sequential it si implemented in this way:
![[Pasted image 20231212174338.png]]
In the sequential case we have that each task execute after the one related to the previous element in the collection ends.
In the parallel case instead we have that each calls of the elemental function run together:
![[Pasted image 20231212175036.png]]
In the above images we are assuming that all the task take the same time but this is not always the case: can happen that the time taken from the task depends from the input so you need a barrier a the end to wait all task to finish.

### N-ary Map
The map we have seen until now is called **unary map**, where we have 1 input collection and 1 output collection.
Some case it is useful to have several input collections, and this type of map is called **n-ary maps**.
**Example**:
![[Pasted image 20231212180045.png]]

### Sequence of maps Optimization
Now we will see some optimization in the case we have a sequences of maps like this:
![[Pasted image 20231212180512.png]]
A naive implementation of this may just write in a intermediate result to memory. This is a very easy implementation but in this way you will waste memory Band Width  and likely overwhelming the cache (this badly affect performance).

#### Code fusion
This implementation will fuse the elemental function of the several map into one big elemental function. This will adds arithmetic intensity but reduces the memory/cache usage. 
![[Pasted image 20231212181332.png]]
This will improve the performance if you can implement the big elemental function that use only the registers and do not rely on memory, if you have a lot of memory involved this is more and less useless.
#### Cache fusion
Sometimes you cannot fuse together the map operations. What you can do is split the work into block, giving one block for each cpu.
![[Pasted image 20231212182040.png]]
Hopefully, since each cpu work only with one block it will find the result of a map step in the cache and it can use it for the next map step.
Since caches are faster than memory this will have better performance from the naive one.

//TODO stencil, work pile, divide and conquer\

### Scaled Vector Addition (SAXPY)
**The problem**:
- We have two vector x and y
- we want to scale x by a factor a, sum it element by element with y and store it in y
y= ax + y
If you think about it each element in vector x and y are independent.
![[Pasted image 20231212183030.png]]
Serial Implementation:
![[Pasted image 20231212185150.png]]
**[[OpenMP]] implementation**:
![[Pasted image 20231212185112.png]]

Performance of OpenMP SAXPY
![[Pasted image 20231212184611.png]]
If you see with 2 thread you do not have the half of the second because of the thread management. With more of 4 you have a slow down because of the limitation of thread that you can tun together (in this case 4) and for the overhead of the thread management.

## Collective Patterns
**Collective operations** are operations that deal with a collection of data as a whole, rather than a separate elements.
Collective patterns include:
- Reduce
- Scan
- Partition
- Scatter
- Gather
### Reduce 
The reduce pattern is used to combine a collection of elements into **one summary value**.
In order to obtain the summary value there is a **combiner function**, that combine elements pairwise.
The combiner function needs to be **associative** in order to parallelize this pattern.
Example of combiner function:
- addition
- multiplication
- max/min
**Parallel Reduce**:
![[Pasted image 20231213121305.png]]
If the the combiner function is associative we can have several task that combine together a smaller portion of the collection, obtaining partial result. Then those partial result are combined together using the same function and so on until we get the final result.
The best way to implement the parallel reduce is a balanced tree: If you have enough processor you will have log(#processor) complexity.


Another good property is **commutativity**: you can change the order of the operation reducing the synchronization issue (as soon as you have 2 ready result you can combine them).

A problem with the reduction 

With the reduce pattern you may also exploit **vectorization**, exploiting vector operation/ multiple alu: with this you have that the same instructions will execute the reduction function on more than 2 item of the collection.

**Tailing** is a technique that you can apply when dealing with big collections: You can divide the collection in chunk of work, give each chunk to a worker that reduce it in a serial way. the workers work in parallel. Then the result of each workers is reduced again:
![[Pasted image 20231213125647.png]]
A problem with the reduction pattern is the **order sensitivity** of floating point operation: if we reduce floating point number in different error you can obtain not identical result, and you have to accept some error.
#### Fuse reduce and map
A common strategy is to fuse the reduce and map pattern, meaning that you first apply the map on a collection and then you will reduce it:
![[Pasted image 20231213125815.png]]
Doing a thing similar to the [[#Parallel patterns#Code fusion|code fusion]] we can combine the two pattern obtaining a single one to reduce the bandwidth usage:
![[Pasted image 20231213130236.png]]
**Example**: DOT product
You have 2 vector of the same length:
- you want to multiply them element by element using map
- then you want to combine the result using +
### Scan
