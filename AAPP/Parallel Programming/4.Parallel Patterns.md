A **Parallel patterns** is a recurring combination of task distribution and data access that solves a specific problem in parallel algorithm design. 
The patterns can help us to create a good parallel algorithm: sometime we can nest together patterns to obtain our algorithm.
![[Pasted image 20231212155339.png]]

Nesting is useful because allows other patterns to be composed
in a hierarchy so that any task block in the above diagram can be replaced with a pattern with the same input/output and dependencies.
We can think at the nesting as a Pattern itself.
# Serial Control patterns
Pattern does not exists only for Parallel programming, but also for Serial programming.
Those are the most important serial control patterns:
- Sequence
- Selection
- Iteration
- Recursion
It is important to know and understand them because some time (if the [[3.Dependencies in Parallel execution|dependencies]] allow us to do that) we can parallelize them.
## Sequence 
The **Sequence** is a list of task that is executed in a specific order:
![[Pasted image 20231212160235.png]]
The specification of the task says that all the statements must be executed together, but if there are no dependencies between some task we can execute them in a parallel way:
![[Pasted image 20231212160352.png]]
## Selection
**Selection** is a very common pattern: we have a condition **c** that is first evaluated. Then, depending on the result of the condition we execute either task **a** or **b**

If the task a and b does not depend on the execution of c we can parallelize this executing a,b,c together and then choose the output depending on the result of c (this is a common practice in hardware design).
![[Pasted image 20231212161706.png]]

## Iteration
In **iteration** a condition c is evaluated. If true, a is evaluated and c is evaluate again. this repeats until c is false.

We have already seen that parallelize this can arise problem when there are [[3.Dependencies in Parallel execution#Loop with dependencies |loop with dependencies]].
![[Pasted image 20231212161724.png]]

## Recursion
Dynamic form of nesting allowing functions to call themselves.
Tail recursion is a special recursion that can be converte into iteration, allowing us to parallelize it.
We can also parallelize several invocation of the function like in the [[3.Divide and Conquer#Merge sort||merge sort]].


# Parallel patterns
Parallel patterns extends serial control patterns. There are several parallel patterns and we will go through each of them.

## Fork-join
The **fork join** pattern allows to split the control flow into multiple flow that rejoin later.
An example of implementation is the one of **click plus**:
1. The call tree is a parallel call tree and where functions are spawned instead of called
2. Function that spawn another function call will continue to execute
3. caller **syncs** with the spawned function to join the two
Notice that the syncs is different than a barrier: in syncs only one thread will continue, with a barrier all thread will continue.
![[The-fork-join-model-used-for-thread-level-parallelism-in-OpenMP.png]]

## Map
The pattern **map** work on a collection of elements: it will perform a function on every element of the collection.
Map replicates a serial iteration pattern, where:
- each iteration is independent from the others
- the number of iteration is known in advance 
- computation only depends on the iteration count and data from input collection
The replicated function is referred to as **elemental function**.
The map pattern can be applied if the elemental function can be applied to each element without knowledge of the neighbor.
Also the elemental function must be **pure**, meaning that it must not modify the shared state.

**Examples**:
![[Pasted image 20231212170848.png]]
The map pattern is embarrassingly parallel, since there is the lack of dependences we can run all the invocation of the elemental function in parallel obtaining a significant speedup:
If we think at the [[4.Parallel Machine Model#Amdahl's law|Amdahl's law]] there is no fraction related to the serial execution. 

### Sequential vs parallel map
If we think a map as a sequential it si implemented in this way:
![[Pasted image 20231212174338.png]]
In the sequential case we have that each task execute after the one related to the previous element in the collection ends.
In the parallel case instead we have that each calls of the elemental function run together:
![[Pasted image 20231212175036.png]]
In the above images we are assuming that all the task take the same time but this is not always the case: can happen that the time taken from the task depends from the input so you need a barrier a the end to wait all task to finish.

### N-ary Map
The map we have seen until now is called **unary map**, where we have 1 input collection and 1 output collection.
Some case it is useful to have several input collections, and this type of map is called **n-ary maps**.
**Example**:
![[Pasted image 20231212180045.png]]

### Sequence of maps Optimization
Now we will see some optimization in the case we have a sequences of maps like this:
![[Pasted image 20231212180512.png]]
A naive implementation of this may just write in a intermediate result to memory. This is a very easy implementation but in this way you will waste memory Band Width  and likely overwhelming the cache (this badly affect performance).

#### Code fusion
This implementation will fuse the elemental function of the several map into one big elemental function. This will adds arithmetic intensity but reduces the memory/cache usage. 
![[Pasted image 20231212181332.png]]
This will improve the performance if you can implement the big elemental function that use only the registers and do not rely on memory, if you have a lot of memory involved this is more and less useless.
#### Cache fusion
Sometimes you cannot fuse together the map operations. What you can do is split the work into block, giving one block for each cpu.
![[Pasted image 20231212182040.png]]
Hopefully, since each cpu work only with one block it will find the result of a map step in the cache and it can use it for the next map step.
Since caches are faster than memory this will have better performance from the naive one.

//TODO stencil, work pile, divide and conquer\

### Scaled Vector Addition (SAXPY)
**The problem**:
- We have two vector x and y
- we want to scale x by a factor a, sum it element by element with y and store it in y
y= ax + y
If you think about it each element in vector x and y are independent.
![[Pasted image 20231212183030.png]]
Serial Implementation:
![[Pasted image 20231212185150.png]]
**[[OpenMP]] implementation**:
![[Pasted image 20231212185112.png]]

Performance of OpenMP SAXPY
![[Pasted image 20231212184611.png]]
If you see with 2 thread you do not have the half of the second because of the thread management. With more of 4 you have a slow down because of the limitation of thread that you can tun together (in this case 4) and for the overhead of the thread management.

## Collective Patterns
**Collective operations** are operations that deal with a collection of data as a whole, rather than a separate elements.
Collective patterns include:
- Reduce
- Scan
- Partition
- Scatter
- Gather
### Reduce 
The reduce pattern is used to combine a collection of elements into **one summary value**.
In order to obtain the summary value there is a **combiner function**, that combine elements pairwise.
The combiner function needs to be **associative** in order to parallelize this pattern.
Example of combiner function:
- addition
- multiplication
- max/min
**Parallel Reduce**:
![[Pasted image 20231213121305.png]]
If the the combiner function is associative we can have several task that combine together a smaller portion of the collection, obtaining partial result. Then those partial result are combined together using the same function and so on until we get the final result.
The best way to implement the parallel reduce is a balanced tree: If you have enough processor you will have log(#processor) complexity.


Another good property is **commutativity**: you can change the order of the operation reducing the synchronization issue (as soon as you have 2 ready result you can combine them).

A problem with the reduction 

With the reduce pattern you may also exploit **vectorization**, exploiting vector operation/ multiple alu: with this you have that the same instructions will execute the reduction function on more than 2 item of the collection.

**Tailing** is a technique that you can apply when dealing with big collections: You can divide the collection in chunk of work, give each chunk to a worker that reduce it in a serial way. the workers work in parallel. Then the result of each workers is reduced again:
![[Pasted image 20231213125647.png]]
A problem with the reduction pattern is the **order sensitivity** of floating point operation: if we reduce floating point number in different error you can obtain not identical result, and you have to accept some error.
#### Fuse reduce and map
A common strategy is to fuse the reduce and map pattern, meaning that you first apply the map on a collection and then you will reduce it:
![[Pasted image 20231213125815.png]]
Doing a thing similar to the [[#Parallel patterns#Code fusion|code fusion]] we can combine the two pattern obtaining a single one to reduce the bandwidth usage:
![[Pasted image 20231213130236.png]]
**Example**: DOT product
You have 2 vector of the same length:
- you want to multiply them element by element using map
- then you want to combine the result using +

#### Merge sort as reduction
e can sort an array via a pair of a map and reduce:
- map each element into a vector containing just that element
1. ![[Pasted image 20231213160228.png]]
2. ![[Pasted image 20231213160310.png]]
3. ![[Pasted image 20231213160327.png]]
<>= merge operation
How fast is this? O(n<sup>2</sup>)

Another way, **Three shaped sort**
![[Pasted image 20231213160505.png]]
even if we had only a single processor this is better:
- We do O(log(n)) merges that takes O(n), with a complexity of O(n log(n))
### Scan
The scan pattern produce a partial reduction of input sequence, generating a new sequence.
There are 2 type of scan:
- **Inclusive scan**: include the current element in the partial reduction, partial reduction is of all element prior to the current one and the current one
- **Exclusive scan**: exclude current element in partial reduction, partial reduction is of all element prior to the current one.

![[Pasted image 20231213154934.png]]
One algorithm for parallelizing the scan pattern is to perform an **up sweep** and a **down sweep**:
- on the up sweep you reduce the input
- The down sweep produce the intermediate results
![[Pasted image 20231213155320.png]]
Scan can exploit both **multiple process** parallelism and **vectorization**:
![[Pasted image 20231213155613.png]]
This  is called **three phase scan with tiling**, when you have multiple thread using the vector instruction in order to compute the scan.

#### Fuse Scan and map
You can fuse scan and map pattern. you can add a map on the input collection or on the output collection and also obtain a chain of scan-map:
![[Pasted image 20231213155931.png]]
Also here you can optimize things using code fusion.

### Gather
**Gather** pattern create a collection of data by reading from another input collection:
- Given a collection of ordered index it read from the source collection at each index and write to the output collection in index order.
The input and the output collection are made from the same type of element.
The output collection and the index collection have the same dimensionality.

How gather is implemented in serial code:
![[Pasted image 20231214141202.png]]
If you see there is a lot of possibility in term of parallelization:
1. **exclusive read on idx**, each iteration read a different element of idx
2. **common read on a**
3. **exclusive write on A**
We can parallelize the for loop to have each iteration running in parallel!
![[Pasted image 20231214143006.png]]

#### Shifts
Shift is a special case of the gather where the index array is implicit:
![[Pasted image 20231214143252.png]]
As we can se from the above picture there are variant that manage out of bounds data at the edge of the array in several way:
- Add a default value
- Duplicate the nearest element
- rotate, putting the out of bound element at the beginning
Shift can take advantage both of vector instruction and from good data locality.
Another thing is that the calculation of the index array can be done in parallel: it is just a map of the input collection, where the elemental function just associate for each element the next position (e.g. elem_pos +1).

#### Zip
Zip is another special case of the gather where the index array is composed in a special way and there are 2 input array:
- it takes the odd position from in array 1
- the even position from in array 2
This is easy to parallelize, just do as in a normal gather and add the even/odd logic.
![[Pasted image 20231214144740.png]]
more:
- can be generalized to more input collection
- can zip data of unlike tipes

usage: combining real and imaginary part of imaginary number

#### Unzip
Unzip take a sequence of complex organized number as pairs (like real numbers) and put the even position in one output array and the odd position in another output array:
![[Pasted image 20231214145731.png]]
This can be parallelized just like a normal gather

### Scatter
**Scatter** takes as input a collection of data and a collection of index and then write in an output collection using the collection of index:
- data collection and index collection have the same dimensionality
![[Pasted image 20231214150316.png]]
The problem with scatter, as we can see from the serial implementation, is that we have concurrent write on the output collection since the element in the index collection can overlap.
![[Pasted image 20231214150426.png]]
What we need are rules to resolve this collision:
- **Atomic Scatter**: Is a non deterministic approach, we do not care on who write the final value. The only things that matter is that read and write will be done atomically in order to have a consistent value![[Pasted image 20231214150713.png]]
- **Permutation Scatter**: Just say that collision are illegal, so the output is a permutation of the input
- **Merge Scatter**: if there is a collision an associative and commutative operator is used to merge the two element (like addition)
- **Priority Scatter**: to every element in the input array is assigned a priority based on its position. This priority is used t decide which element is written in case of collision.

#### Converting Scatter into Gather
Scatter is more expansive than gather, so if we have a **permutation scatter** we can check for collision in advance and convert the address for a scatter into those for a gather.

### Pack
**Pack** is a pattern use to eliminate unused elements from a collection. Retained elements are moved so they are contiguous in memory.
![[Pasted image 20231214171553.png]]**The algorithm**:
1. convert input array of booleans into 0 and 1
2. exclusive scan of this array with the addition operation
3. write values to output array based on offsets 
![[Pasted image 20231214171957.png]]
notice that the obtained array is an additional collection, you still have the original one (with the boolean value) and you can use that to understand which element you have to write and which not.
How to parallelize Pack? **it is just a map plus a scan and a final map**:
- map boolean to int
- exclusive scan int array with sum
- 3-ary map that takes as input the boolean array, result of scan for the position and the input collection and for each element of the input collection if the corresponding boolean is 1 it will write it in the output collection following the result of the exclusive scan.
#### Unpack
**Unpack** is the inverse operation of Pack:
- Given the same data on which element were kept and which were discarded it spread the element back in their original position.
Sequentially you have just to go through the boolean array, if there is a 1 you write in the output array in the current position of the boolean array and start to consider the next element of the input collection.
to parallelize unpack we can:
- Do an exclusive scan on the integer/boolean array calculating the position
- do a map and write the element in the corresponding position if there is a 1 otherwise put a null element in that position
so it is a map + a scan + a map

We can see:
- Pack as a scatter
- unpack as a gather

#### Split and Unsplit
**Split** is the generalization of the pack pattern:
- elements are moved to upper or lower half of the output collection based on some state
- here we do not lose information like in pack
![[Pasted image 20231214173817.png]]
To parallelize this you can compute index using scan and then write the element using map

**Unsplit** is the inverse of split:
- create output collection based on input collections, it put all the elements in their original position taking as input the categorization
![[Pasted image 20231214174027.png]]

**Bin**: you have more than 1 category

#### Map and Pack Fusion
Fuse map and pack can be advantageous if most of the elements of a map are discarded:
- map  checks pair for collision
- pack stores only the actual one


#### Expand
When we apply a map and each element of the array can output any nu,ber of elements we can use **expand** to fuse result together in order.
It is basically a map + an unsplit
![[Pasted image 20231214175309.png]]


## Array of Struct vs Struct of Array

**Array of Struct**: 
- most logical organization layout 
- extremely difficult to access memory for reads and writes (gather and scatter)
- prevents efficient vectorization
- May lead to better cache utilization if data is accessed randomly

**Struct of Array**:
- separate arrays for each structure-field keeps memory acces contiguous when vectorization is performed over structure instances (better for parallelization )
- 

SOA is also better for data layout (need less padding)
![[Pasted image 20231214180225.png]]




