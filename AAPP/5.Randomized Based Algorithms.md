
The randomize algorithm are algorithm that will run differently depending on the result of random "coin tosses".
![[Pasted image 20230928164007.png]]
In those algorithm is likely that they work well on the average case, so they shuffle the data in someway to make it similar to the average case.
This algorithms make the worst case very unlikely, for example shuffling the data.

When analyzing randomized algorithms, several essential tools are frequently employed:

- **Indicator Variables:** These are used to simplify the analysis of complex random variables by breaking them down into individual events or outcomes. They help isolate specific events of interest. X=sum<sub>i</sub> (X<sub>i</sub>)
- **Linearity of Expectations:** This principle allows us to calculate the expected value of the sum of random variables by adding their individual expected values. It simplifies the process of calculating the overall expected performance of an algorithm. If z=x+y -> E[z]=E[x]+E[y]
- **Recurrence Relations:** These mathematical relationships are often used to describe how the performance of an algorithm or a process depends on its previous states or iterations. They are particularly useful for understanding how an algorithm's behavior evolves over time. (as we seen for standard algorithm)


# Hiring problem and Generating random permutation

## The problem
n the hiring problem, the goal is to hire the best candidate while minimizing hiring and firing costs. The headhunter sends you 'n' applicants, one at a time. You need to decide whether to hire or reject each applicant based on their quality. Once you hire an applicant, you must fire the current employee (if any) and hire the new one. The cost of hiring and firing is expensive, and the aim is to minimize the overall cost.

To analyze this problem using expected value and indicator variables:

1. **Setup:**
    
    - Define 'n' as the number of applicants.
    - Assume 'n' applicants arrive in a certain order.
2. **Random Variables and Indicator Variables:**
    
    - Define a set of indicator random variables, denoted as Xi, where i varies from 1 to n. Each Xi represents whether or not the 'i-th' applicant is hired. It takes the value 1 if hired and 0 if rejected.
3. **Expected Value of Hiring:**
    
    - The number of applicants hired, denoted as 'X,' is the sum of all the Xi values: X = ΣXi (from i=1 to n).
4. **Expectation of Xi:**
    
    - The expectation of Xi, E[Xi], represents the probability that the 'i-th' applicant is hired. This probability depends on the quality of the applicants and the order in which they arrive.
5. **Expected Value of Total Cost:**
    
    - The total cost in terms of hiring and firing is proportional to the number of hires. Therefore, you're interested in calculating the expected value E[X], which is the expected number of hires.
6. **Calculating E[X] :**
    
    - To calculate E[X], you need to sum up the expectations of individual Xi values:
        - E[X] = E[X1 + X2 + ... + Xn]
        - By the linearity of expectations, E[X] can be expressed as the sum of individual expectations: E[X] = E[X1] + E[X2] + ... + E[Xn].
7. **Determining E[Xi] :**
    
    - The probability that the 'i-th' applicant is hired, E[Xi], depends on the order of applicants. If there are 'i' applicants who are better than the previous ones, the probability that the 'i-th' applicant is hired is 1/i.
    - This means E[Xi] = 1/i.
8. **Expected Value of Total Hires:**
    
    - So, the expected number of hires, E[X], is calculated by summing these probabilities over all 'n' applicants:
        - E[X] = 1/1 + 1/2 + 1/3 + ... + 1/n.
9. **Simplifying E[X] :**
    
    - This expression is essentially the sum of the harmonic series, which can be approximated as ln(n) + O(1). Therefore, the expected number of hires is approximately ln(n).

In summary, the expected number of hires in the hiring problem, analyzed using the concept of expected value and indicator variables, is approximately the natural logarithm of 'n.' This analysis provides insights into how the problem behaves on average across different orderings of applicants, allowing you to make informed decisions while minimizing hiring and firing costs.

If we causally sort how we send the applicants and we want to see how many times the third applicant is better than the previous we can easily see that this do not happens in the majority of the case:
![[Pasted image 20231023121015.png]]
so in the majority of the case we are in a optimal case.

This property to not fall in the worst case in the majority of the times can be found in more and less all the randomized algorithm, since they actively change the input to make the average case most likely to happens!

# Las Vegas and Monte Carlo
There are two types of randomized based algorithm:
1. Las Vegas
2. Monte Carlo

The main difference between this two category is related to the results they output: 
- Las Vegas algorithms always gives the correct solutions, the only thing that change from a run to another is the running time
- Monte Carlo may sometimes produce a incorrect solution, but we are able to bound the probability of such uncertainty
We can further divide Monte Carlo algorithms in 2 classes:
- **One-sided error**: probability that the output is zero at least for one of the possible output (yes=yes /no=maybe no maybe yes)
- **Two-sided error**: no zero probability that the output is incorrect for each of the possible output

**Which is better, Monte Carlo or Las Vegas?**
- The answer depends on the application - in some applications an incorrect solution may be catastrophic.
- A Las Vegas algorithm is by definition a Monte Carlo algorithm with error probability 0. 
- A Las Vegas algorithm is an efficient Las Vegas algorithm if on any input its expected running time is bounded by a polynomial function of the input size.
- A Monte Carlo algorithm is an efficient Monte Carlo algorithm if on any input its worst-case running time is bounded by a polynomial function of the input size.

## Search with Randomized based Algorithm
### Monte Carlo
Monte Carlo Algorithm: Finding 'a' in Array A

**Input:**
- `A`: Array of elements
- `n`: Number of elements in the array
- `k`: Number of attempts to find 'a'

**Output:**
- The probability of finding 'a' after k attempts

```python
function findingA_MC(array A, n, k)
    i = 1
    repeat
        Randomly select one element out of n elements.
        i = i + 1
    until i = k or 'a' is found
end
````
This algorithm does not guarantee success, but the runtime is fixed at O(k) because it always makes exactly `k` attempts to find 'a'.
The probability of finding a is:
p(find 'a')=1 - 1/2<sup>k</sup>
### Las Vegas

**Input:**
- `A`: Array of elements
- `n`: Number of elements in the array

**Output:**
- Success when 'a' is found

```python
function findingA_LV(array A, n)
    repeat
        Randomly select one element out of n elements.
    until 'a' is found
end
````
This Las Vegas algorithm aims to find the element 'a' in the given array `A`. It repeatedly selects an element from the array at random until it finds 'a'. The algorithm succeeds with a probability of 1 (i.e., it guarantees success). However, the running time is not fixed and may vary depending on how many attempts it takes to find 'a'. The running time is random and can be expressed as a random variable.
# Kager's min-cut algorithm
## The problem
Given a connected undirected graph G=(V,E) (no direction on the arc and there is a path between each node), where V is the set of the vertices and E is the set of the edge.
Let n= |V| and m=|E|.
Taking a subset S of V, we define **cut(S)={(u,v) ∈ E: (u ∈ S) and !(v ∈ S)}**.
informally the cut is the set of edge that if removed divide the vertices in S from the vertices not in s (two separated graph are created).
The goal of Kager's algorithm is to find a cut of minimum size.
![[Pasted image 20231024110614.png]]
### Minimum st-cut problem
The minimum st-cut problem is a specific variant of the min cut problem, it gives us two vertices s and t and the goal is to find a minimum cut where s in one of the two new graph and t is in the other.

This was used to solve the min-cut problem, by solving n-1 min-st-cut problem:
once you solved for n-1 pair of vertices, you are going to find the solution of the standard min-cut problem because you are considering all possible cases starting from a vertices s to all other one.
The fastest algorithm for solving max-st-flows run in **O(nm log(n<sup>2</sup> /m))**, that has to be run n-1 time. notice that the those computation can be done simultaneously with the same time bounds.

## Kager's Approach
Kager found a smart way to solve the min cut problem, using a Monte Carlo approach (no guarantee that the final solution will be the minimum cut).
Not have a minimum solution is not a big deal, we will obtain a solution not a lot bigger than the smallest but in few time. 
In addition the Kager's algorithm is easier to be implemented.

### The input
The input of the Kager's Algorithm is a multi-edges graph, with no self loop (if there is one can be removed).
![[Pasted image 20231024114557.png]]
The self loops are not relevant since the cut need to consider edges between 2 different vertices.

### Contraction
The Contraction is a key operation in the algorithm. 
The contraction work on edge:
1. Select an edge e=(u,v)
2. build a new graph that has exact the same edges as before, but instead of vertices u and v you are going to have only one vertices uv and for every edge connected to u or v in the original graph there is a edge to uv in the new graph
**Example**: contracting edge e=(1,2)

![[Pasted image 20231024114557.png]]
![[Pasted image 20231024115421.png]]
if you keep contracting you end to have only two vertices in the graph,that represent the solution of the cut of the problem!

### The algorithm
1. start from initial graph
2. contract an edge (chosen at random)
3. repeat until only two vertices remain (n-2 edges contracted)

These two vertices correspond to a partition (S, S’) of the original graph,
and the edges remaining in the two vertex graph correspond to cut(S) in the
original input graph.

**Is this the minimum cut?** There is a lemma that prove that this algorithm find the best solution with a probability >= 1 / (<sup>n</sup><sub>2</sub>)   


Repeating the algorithm can be useful to increase the probability. 
In fact if we repeat the algorithm "l (<sup>n</sup><sub>2</sub>)" times the probability that at least one run found the minimum cut is:
![[Pasted image 20231024124231.png]]
and so, if we set l=c log(n) we can have an error probability <1/n<sup>c</sup>.

### complexity
one run of Karger's algorithm takes O(n<sup>2</sup>) time.
If we repeat l (<sup>n</sup><sub>2</sub>) time, (<sup>n</sup><sub>2</sub>) is on the order of n<sup>2</sup> so we have a randomized algorithm take O(n<sup>4</sup> log(n)) with an error 1/poly(n).

### Conclusion
- The algorithm is very simple to implement
- More and less the same complexity (a bit more) of the solution using the st-flows
- a good probability of obtain the right solution

### Karger and Stein algorithm 
Is a faster Version of the previous algorithm. 

#### Idea
At the beginning of the Karger's algorithm is very unlikely that an edge of the minimum cut is contracted, but in the last steps this probability grows.

in particular, for a fixed minimum cut cut(S), the probability that this cut survives down to vertices is at least (<sup>l</sup><sub>2</sub>)/(<sup>n</sup><sub>2</sub>).
If l = n/sqrt(2) the probability of get the minimal solution is 50%.

This can be exploited to make the algorithm do not do the whole contraction process:
- you chose a trash hold, for example 6
- if the number of vertices is less than six you call the normal Karger algorithm
- else you call recursively this algorithm twice (in this part you are exploiting the 50% probability to have a minimal solution) and you will return the minimum of the 2 solution

#### Pseudo Code
```markdown
1. Procedure contract(G=(V,E), t):
    - while |V| > t
        - choose e ∈ E uniformly at random
        - G = G/e
    - return G

2. Procedure fastmincut(G=(V,E)):
    - if |V| < 6:
        - return mincut(V)
    - else:
        - t = ⌈1 + |V|/√2⌉
        - G1 = contract(G, t)
        - G2 = contract(G, t)
        - return min {fastmincut(G1), fastmincut(G2)}
```
#### Complexity
This is a recursive algorithm, so we can use a recurrence equation:
**T(n) = 2 n<sup>2</sup> + T(n/sqrt(2))**
2n<sup>2</sup> is the complexity of the contraction procedure. 
and solve it with the [[2. Analysis of algorithms#The Master Method |Master Method]]:
a=2
b=sqrt(2)
f(n)= contract()
log<sub>b</sub>(a)~2
f(n) is O(n<sup>2</sup>), so we are in case 2 of master theorem, with k=1.
T(n)= O(n<sup>2</sup> log(n)) -> this is the complexity for a single run of the algorithm!

#### Probability to succeed
![[Pasted image 20231024164657.png]]
So with a complexity O(n<sup>2</sup> log<sup>3</sup> (n)) we have a probability of success >= 1- 1/poly(n)
