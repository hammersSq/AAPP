# Key points
- what is a machine model
-  why do we need a machine model
- RAM MODEL
- PRAM MODEL
- analyzing vector matrix multiplication with pram
# Performance evaluation on parallel algorithms
We want to do an analysis taking in mind that the program is executed in parallel from different executor.
In order to be abstract we use a model called Parallel Machine Model. 
## Parallel machine model
**The PRAM Model**: The model describe a machine, it put a value to the operation on the machine. A model is useful because makes easy to reason algorithms, to achieve complexity bounds, to analyse maximum parallelism. operation

**The RAM machine**
The RAM is the machine model on which the PRAM is based
**What do we have in RAM?**  
![[Pasted image 20230921171453.png]]

**The PRAM** (Parallel random access machine),is an abstract machine for   for designing algorithms applicable to parallel computers. 
it is a set of replicated ram machines:
![[Pasted image 20230921172120.png]]
The shared memory cells are used from the various machine to communicate which each other. 
![[Pasted image 20230928142011.png]]
a lot of simplification are done: the memory is shared, but we do not care??
execution mode: the same program for each processor (single program multiple data), follow the 5 stages:
1. read all values from the input, WHEN ALL PROCESSOR ready GOTO 2
2. read data of the shared memory cells, when all ready goto 3
3. perform internal computation, when all ready goto 4
4. may write into one of the output cells
5. may write into one of the shared memory cells
if we need to have process doing different things we can decide to switch off some process, and do not analyze it 


Memory in PRAM
![[Pasted image 20230921173132.png]]

how to manage write conflicts? we solve it in the phase of real implementation, but when studying complexity is not important (not sure)


Below are shown several configuration of the PRAM, based on read/write capabilities.
![[Pasted image 20230921173433.png]]
there are possible approach to address concurrent write:
1. priority CW: processor have priority based on which value is decided, the highest priority is allowed to complete write
2. common cw: all processor are allowed to complete write if and only if all the value to be written are equals 
3. arbitrary/random cw: one randomly chosen processor is allowed to complete write



![[Pasted image 20230921173833.png]]
 PRAM is able to say how many CPUs are needed to execute a program (max cpu to have better performance)


**computational power**: model A is computational stronger than a model B(A>=B) IFF any algorithm written for B will run unchanged on A in the same parallel time and the same basic properties.


Some definition used to classify algorithms   
![[Pasted image 20230921174221.png]]  

![[Pasted image 20230928143015.png]]
**Massively parallel execution**: application with no sequential part, in that case you are going to see a linear speedup

## Matrix Vector Multiply 
We want to describe an algorithm and to analyze it for matrix vector multiplication in PRAM.
![[Pasted image 20231021215531.png]]
each processor is going to work on r=n/p rows of the matrix. the maximum number of useful processor are n, the number of row!
**Example of the algorithm**:
![[Pasted image 20231021215821.png]]
the "i" on the left mean that that operation is done by each processor, but they are working on its own part of the data
Graphic visualization of the algorithm:
![[Pasted image 20231021220135.png]]
**Complexity**:
![[Pasted image 20231021220211.png]]
This algorithm has linear speedup, it is massively parallel.

### The idea of the algorithm
![[Pasted image 20230928143252.png]]



## Sum of a sequence of number
The sum of a sequence of number is a trivial problem, the goal is to take a sequence of number and to calculate their sum.
With a sequential algorithm to solve this problem we clearly need constant time, but in a parallel world (in our case, using the pram machine) we can write an algorithm that need less computational time!

### Logarithmic sum
This algorithm take as input a vector of element,and output the sum of all elements.
![[Pasted image 20230926144057.png]]
**How does it works?** 
1. The array `B` is initialized with the same values as array `A`. Initially, `B(I)` is set to `A(I)` for each `I`.
2. The algorithm then runs a loop for a logarithmic number of times based on `LOG(N)`, where `N` is the size of the array. 
3. Inside the loop, the algorithm checks whether the index `I` is less than or equal to `N / 2^H`. This is an important step to determine which elements in the array will be summed together.
4. If `I` is less than or equal to `N / 2^H`, it calculates the value of `B(I)` as the sum of its two "children" in the tree. In practice, `B(I)` is computed as the cumulative sum of elements in the corresponding range in the array `A`.
**Analysis**
![[Pasted image 20231022185452.png]]
**What about we have a number of data that is larger than the number of processor?**
![[Pasted image 20231022185511.png]]

![[Pasted image 20231022185548.png]]

# Matrix Multiplication on PRAM
![[Pasted image 20231022190452.png]]
we can massive parallelize the matrix multiplication, each cell is independent from the others so we can calculate all the cells all together.
for each cells we need n processors in order to parallelize the multiplication.
to have the best performance you need **n<sup>3</sup>** processors, because you have n<sup>2</sup> cells, so you can have a processor per cell. In addition you can parallelize the work done to calculate each cells, so you have n multiplication that can run in parallelize. 
Matrix multiplication is not a massively parallel problem, because you still have the reduction, you do not need all the processor you used for the multiplication part to do the reduction: so you have some inefficiency (not all processors run at the same time).

**The PRAM algorithm**![[Pasted image 20231022192048.png]]
- **step 1**: you have to compute all the possible combination in term of multiplication, (T<sub>i,j,l</sub> means that i need n<sup>3</sup> processors and these processors are identified by 3 index. T<sub>i,j,l</sub> takes the data from A<sub>i,l</sub> and B<sub>l,j</sub> ). all the data about the multiplication are stored in a 3-dimension temporary matrix, T. this part of the algorithm is fully paralle. **This step takes 1**
- **step 2**: after step 1 you have to do the reduction: summing up the pair of multiplication that we obtain by row column. this is done by building a binary tree computation (like logarithmic sum).  at the end of the computation the result is in the first position of the vector. **This step takes log<sub>2</sub>(n)**
- **step 3**: store the first element of each row/column in T (the sum of the multiplication) in the matrix result. **this step takes 1**
if p= n<sup>3</sup> so all the things are parallel you are going to take 1 units of time for the first step.
if p<n<sup>3</sup> : n<sup>3</sup>/p ... you have to split the work between the processor and each processor work in a sequential way on its data.
this things is proved in a general way for all parallel algorithm. see following lemmas
## Prefix sum
![[Pasted image 20231022194557.png]]
in a sequential way you need to: take the first value and give it as output, then you add up the second value ecc... this is going to take the same complexity of the reduction of before.
**in the parallel case:** you can built a binary tree:
1. compute: a1+a2, a3+a4, a5+a6, a7+a8.
2. compute the sum of the 4 intermediate result with 2 operations
3. compute the sum of the 2 intermediate results with one operation.
below a graph showing the calculations:
![[Pasted image 20231022195140.png]]
We are using 4 processor in stage1, 2 processor in stage 2 and 1 processor in stage 3.
the idle processors in all phase can be used to calculate other intermediate result, like in pahse 2 a processors acn calculate s1=(a1+a2) plus a3 obtaining s3. in this way i' m able to calculate al the result using all the processors. 
each row in the graph is a operation done by a processors

the work of this algorithm is nlog(n), but the time taken to obtain the result is log(n).
remember that each intermediate result is a Result!!! in this case we want to know each intermediate Si, this is why we calculate them.
# Lemma 1
![[Pasted image 20231022193745.png]]
Just you need to split the work done by the processors P in a lower number of processors P'. so there is the need of serialize this work in the processor (a processor for example has to do the work of 2 processors)
# Lemma 2
![[Pasted image 20231022193805.png]]
The same thing hold for the memory. 
# Amdal and gustafson 
Which law we have to understand the upper-bound of a given computation?
There are two law:
## Amdahl's law
![[Pasted image 20231023095017.png]]
![[Pasted image 20231023095108.png]]
The only part that can be parallelized is the blue part. the time taken for the red part is always the same. In the imagine above is calculated the speedup taking into account this.
The second calculation is the speedup if we have an infinite number of processors, as we can see there is a limit that is determined by the time take from the serial part.

This law is often used to argue that we have not to build parallel architecture, but the Amdahl's law do not take into account a different dimension: **The size of the data**.
## Gustafson's Law
Gustafson understood that if you are going to increase the amount of data that you are working on the sequential part do not become bigger,but the parallel part yes. so we can see a increased speedup only increasing the size of th data that we are working on.
![[Pasted image 20231023100230.png]]
This is called **weak scaling** because with the respect of amdahl' s law the parallelism is lower when the sequential part is small.
This is saying that have parallel architecture is interesting because we have an amount of data that is always increasing.

## Conclusion
1. **Amdahl's Law:** Amdahl's Law is a fundamental principle in computer science that deals with the limitations of improving the speed of a program when you increase the processing power. It states that the overall speedup of a program is limited by the portion of the program that cannot be parallelized or made to run in multiple threads or processors. In simpler terms, if there's a part of a task that can't be split into smaller pieces and run simultaneously, increasing computing power won't make that part faster.
    
    **Example:** If you have a task that's 90% parallelizable and 10% sequential (can't be parallelized), even if you have a super-powerful computer, the maximum speedup you can achieve is limited by that 10% sequential part.
    
2. **Gustafson's Law:** Gustafson's Law is a contrasting idea. It argues that when you have more computing power available, you can use it to handle more extensive or detailed data. In the context of simulations (like the example you provided with nuclear detonations), it means that with more powerful computers, you can include more variables and simulate a situation more comprehensively. Instead of rushing through the simulation due to time constraints, you can take your time to include more aspects and get a more accurate result.
    
    **Example:** In a nuclear detonation simulation, with increased computing power, you can simulate not just the overall explosion but also the effects on individual buildings, cars, and even the contents within them. This leads to a more accurate and detailed simulation.
    

In summary, Amdahl's Law emphasizes the limits of speeding up certain tasks, while Gustafson's Law highlights how increased computing power allows for more comprehensive and detailed analysis or simulations, resulting in better and more accurate results. These laws are important considerations in the world of computer science, especially in high-performance computing and parallel processing.